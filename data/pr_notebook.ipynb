{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f84d6b3",
   "metadata": {},
   "source": [
    "https://github.com/ukcp-data/ukcp-spatial-files/tree/master/spatial-files/ukcp18-uk-land-5km\n",
    "https://catalogue.ceda.ac.uk/uuid/e304987739e04cdc960598fa5e4439d0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb15c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterio import features\n",
    "import rasterio\n",
    "import os\n",
    "from shapely.geometry import shape, box, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "import json\n",
    "import xclim.indicators.atmos as xci\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# def change_from_baseline(data, output, name, period=15):\n",
    "#     first_period = data.isel(year=slice(0, period))\n",
    "#     baseline_mean = first_period.mean(dim='year')\n",
    "\n",
    "#     change_from_baseline = data - baseline_mean\n",
    "\n",
    "#     trimmed_change = change_from_baseline.isel(year=slice(15, None))\n",
    "\n",
    "#     change_path = os.path.join(output, '01_rolling', name)\n",
    "#     trimmed_change.to_netcdf(change_path)\n",
    "# change_from_baseline(merged_ds, output_folder, \"rolling_change_from_baseline.nc\", period=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbce356",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [\"max\", \"min\", \"med\"]\n",
    "rolling_period = 30\n",
    "output_folder = \"./processed/pr\"\n",
    "input_folder = \"./raw/pr\"\n",
    "models = [\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\", \"23\", \"25\", \"27\", \"29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03257b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_extract(model_number):\n",
    "    nc_file_list = glob.glob(os.path.join(input_folder,model_number, \"*.nc\"))\n",
    "    combined_ds = xr.open_mfdataset(nc_file_list, combine='by_coords')\n",
    "\n",
    "    #Calculate SPI\n",
    "    monthly_ds_spi = xci.standardized_precipitation_index(\n",
    "        combined_ds.pr, \n",
    "        window=3, \n",
    "        freq='MS',\n",
    "        dist=\"gamma\",\n",
    "        method=\"APP\",\n",
    "        cal_end=\"2010-12-30\",\n",
    "        fitkwargs={\"floc\": 0},\n",
    "    )\n",
    "\n",
    "    months_below_minus1 = monthly_ds_spi < -2\n",
    "\n",
    "    yearly_avg_spi = months_below_minus1.groupby('time.year').mean(dim='time', keep_attrs=True)\n",
    "    yearly_avg_spi = yearly_avg_spi * 12\n",
    "    yearly_avg_spi = yearly_avg_spi.isel(year=slice(1, None))\n",
    "    rolling_spi = yearly_avg_spi.rolling(year=30, center=True, min_periods=1).mean()\n",
    "\n",
    "    yearly_groups = []\n",
    "    for i, (year, yearly_ds) in enumerate(combined_ds.groupby('time.year')):\n",
    "\n",
    "        # Highest daily rainfall in the year\n",
    "        augmented_yearly_data = yearly_ds.max(dim='time', keep_attrs=True)\n",
    "        \n",
    "        # Total rainfall in the year\n",
    "        yearly_sum = yearly_ds['pr'].sum(dim='time', keep_attrs=True)\n",
    "        augmented_yearly_data['sum'] = yearly_sum\n",
    "        \n",
    "        # Rainfall on wettest 3 day period\n",
    "        rolling_3day_sum = yearly_ds['pr'].rolling(time=3, min_periods=3).sum()\n",
    "        max_3day_sum = rolling_3day_sum.max(dim='time', keep_attrs=True)\n",
    "        augmented_yearly_data['max_3day_sum'] = max_3day_sum\n",
    "        \n",
    "        # Days under 0.2mm rainfall\n",
    "        days_under_1mm = (yearly_ds['pr'] < 0.2).sum(dim='time', keep_attrs=True)\n",
    "        days_under_1mm.attrs = yearly_ds['pr'].attrs.copy()\n",
    "        augmented_yearly_data['days_under_1mm'] = days_under_1mm\n",
    "        \n",
    "        yearly_groups.append(augmented_yearly_data)\n",
    "            \n",
    "    # Concatenate all years along a new 'year' dimension\n",
    "    yearly_groups = yearly_groups[1:]\n",
    "    merged_ds = xr.concat(yearly_groups, dim='year')\n",
    "    combined_ds.close()\n",
    "    merged_ds['rolling_sum'] = merged_ds['sum'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    merged_ds['rolling_max_3day_sum'] = merged_ds['max_3day_sum'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    merged_ds['rolling_pr'] = merged_ds['pr'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    merged_ds['rolling_dry'] = merged_ds['days_under_1mm'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "\n",
    "    rolling_spi = rolling_spi.transpose(*merged_ds['rolling_pr'].dims)\n",
    "    rolling_spi = rolling_spi.assign_coords({\n",
    "        \"projection_y_coordinate\": merged_ds.projection_y_coordinate,\n",
    "        \"projection_x_coordinate\": merged_ds.projection_x_coordinate,\n",
    "        \"year\": merged_ds.year\n",
    "    })\n",
    "    rolling_spi.attrs.update(merged_ds.rolling_pr.attrs)\n",
    "    rolling_spi = rolling_spi.transpose(*merged_ds['rolling_pr'].dims)\n",
    "    merged_ds = merged_ds.drop_vars(['sum', 'max_3day_sum', 'pr', 'days_under_1mm'])\n",
    "    merged_ds['rolling_spi'] = rolling_spi\n",
    "\n",
    "    out_rolling_path = os.path.join(output_folder, '01_rolling', f\"rolling_average_{model_number}.nc\")\n",
    "    merged_ds.to_netcdf(out_rolling_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae90a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_models = models[14:16]\n",
    "\n",
    "for model in select_models:\n",
    "    print(f\"Processing model {model}\")\n",
    "    combine_and_extract(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609913f",
   "metadata": {},
   "source": [
    "### Characterise models\n",
    "\n",
    "For the different model data downloaded, find the median, min and max of the datasets and create 3 new files to use for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329060cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterise_rolling_averages():\n",
    "    # Step 1: Load all NetCDF files\n",
    "    file_paths = glob.glob(\"./processed/pr/01_rolling/*.nc\")  # Replace with your actual directory\n",
    "\n",
    "    # Step 2: Open them as a multi-file dataset (combine along 'ensemble_member')\n",
    "    datasets = [xr.open_dataset(fp, decode_times=False) for fp in file_paths]\n",
    "    combined = xr.concat(datasets, dim='ensemble_member')\n",
    "\n",
    "    # Step 3: Compute statistics across the ensemble dimension\n",
    "    median_ds = combined.median(dim='ensemble_member', keep_attrs=True)\n",
    "    min_ds = combined.min(dim='ensemble_member', keep_attrs=True)\n",
    "    max_ds = combined.max(dim='ensemble_member', keep_attrs=True)\n",
    "\n",
    "    # Step 4: Save each result to new .nc files\n",
    "    median_ds.to_netcdf(\"./processed/pr/01-1_stats/precipitation_median.nc\")\n",
    "    min_ds.to_netcdf(\"./processed/pr/01-1_stats/precipitation_min.nc\")\n",
    "    max_ds.to_netcdf(\"./processed/pr/01-1_stats/precipitation_max.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "characterise_rolling_averages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad9c74",
   "metadata": {},
   "source": [
    "### Manual processing\n",
    "\n",
    "Before running the next steps, load the created .nc file into QGIS, use the batch function to clip the layers, without manually reprojecting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ceae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tif_to_geojson(tif_folder, geojson_folder):\n",
    "    absYears = [50, 60, 70, 80, 90, 100]\n",
    "    os.makedirs(geojson_folder, exist_ok=True)\n",
    "    tif_files = glob.glob(os.path.join(tif_folder, \"*.tif\"))\n",
    "    for tif_file in tif_files:\n",
    "        years = absYears\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            # Try to get timebands from metadata (e.g., tags or band descriptions)\n",
    "            timebands = []\n",
    "            if src.count > 1:\n",
    "                timebands = [src.descriptions[i] if src.descriptions[i] else f\"{i+1}\" for i in range(src.count)]\n",
    "            else:\n",
    "                timeband = src.tags().get('timeband', None)\n",
    "                timebands = [timeband if timeband else \"unknown\"]\n",
    "\n",
    "            # Only keep timebands that match the years list\n",
    "            selected_indices = [i for i, tb in enumerate(timebands) if any(str(y) in str(tb) for y in years)]\n",
    "            for band_idx in selected_indices:\n",
    "                image = src.read(band_idx + 1)\n",
    "                if image.dtype != 'float32':\n",
    "                    image = image.astype('float32')\n",
    "                mask = image != src.nodata\n",
    "                shapes_gen = features.shapes(image, mask=mask, transform=src.transform)\n",
    "                geoms = []\n",
    "                for geom, value in shapes_gen:\n",
    "                    if np.isnan(value) or value == src.nodata:\n",
    "                        continue\n",
    "                    oriented_geom = orient(shape(geom), sign=1.0)\n",
    "                    geoms.append({\n",
    "                        'geometry': oriented_geom,\n",
    "                        'properties': {'value': float(value), 'timeband': timebands[band_idx]}\n",
    "                    })\n",
    "                if not geoms:\n",
    "                    print(f\"No geometries found for {tif_file}, band {band_idx + 1} ({timebands[band_idx]})\")\n",
    "                    continue\n",
    "                gdf = gpd.GeoDataFrame.from_features(geoms, crs=src.crs)\n",
    "                gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "                geojson_path = os.path.join(\n",
    "                    geojson_folder,\n",
    "                    f\"{os.path.splitext(os.path.basename(tif_file))[0]}_{timebands[band_idx]}.geojson\"\n",
    "                )\n",
    "                gdf.to_file(geojson_path, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089dbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_cells_to_features(image, transform):\n",
    "    features = []\n",
    "    rows, cols = image.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            value = image[row, col]\n",
    "            if np.isnan(value):\n",
    "                continue\n",
    "            # Get the bounds of the cell\n",
    "            x0, y0 = rasterio.transform.xy(transform, row, col, offset='ul')\n",
    "            x1, y1 = rasterio.transform.xy(transform, row, col, offset='lr')\n",
    "            geom = box(x0, y1, x1, y0)\n",
    "            features.append({\n",
    "                'geometry': mapping(geom),\n",
    "                'properties': {'value': float(value)}\n",
    "            })\n",
    "    return features\n",
    "\n",
    "def tif_to_geojson(tif_folder, geojson_folder):\n",
    "    absYears = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    os.makedirs(geojson_folder, exist_ok=True)\n",
    "    tif_files = glob.glob(os.path.join(tif_folder, \"*.tif\"))\n",
    "    \n",
    "    for tif_file in tif_files:\n",
    "        years = absYears\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            print(f\"\\nProcessing {tif_file}\")\n",
    "\n",
    "            if src.count > 1:\n",
    "                timebands = [src.descriptions[i] if src.descriptions[i] else f\"{i+1}\" for i in range(src.count)]\n",
    "            else:\n",
    "                timeband = src.tags().get('timeband', None)\n",
    "                timebands = [timeband if timeband else \"unknown\"]\n",
    "\n",
    "            selected_indices = [i for i, tb in enumerate(timebands) if any(str(y) in str(tb) for y in years)]\n",
    "\n",
    "            for band_idx in selected_indices:\n",
    "                image = src.read(band_idx + 1)\n",
    "\n",
    "                if src.nodata is not None:\n",
    "                    mask = ~np.isnan(image) & (image != src.nodata)\n",
    "                else:\n",
    "                    mask = ~np.isnan(image)\n",
    "                geoms = raster_cells_to_features(image, src.transform)\n",
    "                gdf = gpd.GeoDataFrame.from_features(geoms, crs=src.crs)\n",
    "                gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "                geojson_path = os.path.join(\n",
    "                    geojson_folder,\n",
    "                    f\"{os.path.splitext(os.path.basename(tif_file))[0]}_{timebands[band_idx]}.geojson\"\n",
    "                )\n",
    "                gdf.to_file(geojson_path, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd5397ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_max_3day.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_max_dry.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_max_maxPR.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_max_spi.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_max_yearlySum.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_med_3day.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_med_dry.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_med_maxPR.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_med_spi.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_med_yearlySum.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_min_3day.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_min_dry.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_min_maxPR.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_min_spi.tif\n",
      "\n",
      "Processing ./processed/pr\\02_clipped\\PR_min_yearlySum.tif\n"
     ]
    }
   ],
   "source": [
    "# Converts the reprojected and clipped tif files into geojson files at the different time steps\n",
    "\n",
    "tif_to_geojson(os.path.join(output_folder, \"02_clipped\"), os.path.join(output_folder, \"03_geojson\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9abacb",
   "metadata": {},
   "source": [
    "### Inverts JSONS\n",
    "Inverts the storage of data so that it uses lat/lon as keys to reference all the values temporaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347f6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseGeojson(filename, stat):\n",
    "    file_paths = [\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_10.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_20.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_30.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_40.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_50.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_60.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_70.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_80.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_90.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_100.geojson\"),\n",
    "]\n",
    "\n",
    "    file_labels = [\n",
    "        \"1990\",\n",
    "        \"2000\",\n",
    "        \"2010\",\n",
    "        \"2020\",\n",
    "        \"2030\",\n",
    "        \"2040\",\n",
    "        \"2050\",\n",
    "        \"2060\",\n",
    "        \"2070\",\n",
    "        \"2080\"\n",
    "    ]\n",
    "\n",
    "    # Dictionary to store: { coordinate: [value1, value2, ...] }\n",
    "    coordinate_data = defaultdict(lambda: [None] * len(file_paths))\n",
    "\n",
    "    # Process each file\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        with open(file_path, 'r') as f:\n",
    "            geojson = json.load(f)\n",
    "            for feature in geojson.get(\"features\", []):\n",
    "                geometry = feature.get(\"geometry\", {})\n",
    "                properties = feature.get(\"properties\", {})\n",
    "                value = properties.get(\"value\")  # change if your key is different\n",
    "\n",
    "                if geometry.get(\"type\") == \"Polygon\":\n",
    "                    coords = geometry.get(\"coordinates\", [])\n",
    "                    if coords and coords[0]:  # outer ring exists\n",
    "                        raw_coord = coords[0][0]  # first coordinate of outer ring\n",
    "                        # Format coordinate to 10 decimal places, pad with zeros if necessary\n",
    "                        coord_key = tuple([f\"{c:.10f}\" for c in raw_coord])\n",
    "                        coordinate_data[coord_key][idx] = value\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\"Coordinate\": coord, **{file_labels[i]: values[i] for i in range(len(file_labels))}}\n",
    "        for coord, values in coordinate_data.items()\n",
    "    ])\n",
    "\n",
    "    # Optional: remove rows where all values are None\n",
    "    # df = df.dropna(subset=file_labels, how='all')\n",
    "\n",
    "    # Show result\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.join(output_folder, f\"04_inverse/{stat}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df.to_json(os.path.join(output_dir, f\"{filename}_inverse.json\"), orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c8ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stat in stats:\n",
    "    inverseGeojson(f\"PR_{stat}_3day\", stat)\n",
    "    inverseGeojson(f\"PR_{stat}_dry\", stat)\n",
    "    inverseGeojson(f\"PR_{stat}_maxPR\", stat)\n",
    "    inverseGeojson(f\"PR_{stat}_spi\", stat)\n",
    "    inverseGeojson(f\"PR_{stat}_yearlySum\", stat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9502acf",
   "metadata": {},
   "source": [
    "### Merging JSONS for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a048d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphData(stat):\n",
    "    file_paths = {\n",
    "        \"PR_3day\": os.path.join(output_folder, \"04_inverse\", stat,f\"PR_{stat}_3day_inverse.json\"),\n",
    "        \"PR_maxPR\":os.path.join(output_folder, \"04_inverse\", stat, f\"PR_{stat}_maxPR_inverse.json\"),\n",
    "        \"PR_yearlySum\":os.path.join(output_folder, \"04_inverse\", stat, f\"PR_{stat}_yearlySum_inverse.json\"),\n",
    "        \"PR_dry\":os.path.join(output_folder, \"04_inverse\", stat, f\"PR_{stat}_dry_inverse.json\"),\n",
    "        \"PR_spi\":os.path.join(output_folder, \"04_inverse\", stat, f\"PR_{stat}_spi_inverse.json\"),\n",
    "    }\n",
    "    \n",
    "    data_by_metric = {}\n",
    "\n",
    "    # Load each file and organize by coordinate\n",
    "    for metric, path in file_paths.items():\n",
    "        with open(path, 'r') as f:\n",
    "            records = json.load(f)\n",
    "            for entry in records:\n",
    "                # Convert coordinate list to tuple for use as a dictionary key\n",
    "                coord_key = tuple(entry[\"Coordinate\"])\n",
    "                if coord_key not in data_by_metric:\n",
    "                    data_by_metric[coord_key] = {}\n",
    "                # Extract values for 1990 to 2080 in 10-year steps\n",
    "                values = [entry.get(str(year)) for year in range(1990, 2090, 10)]\n",
    "                data_by_metric[coord_key][metric] = values\n",
    "\n",
    "    # Convert coordinate tuples to strings for JSON serialization\n",
    "    final_output = {str(k): v for k, v in data_by_metric.items()}\n",
    "\n",
    "    # Save the final merged dictionary as a new JSON file\n",
    "    output_file = os.path.join(output_folder, f\"05_graphData/{stat}.json\")\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        json.dump(final_output, out_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858ff0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = os.path.join(output_folder, \"05_graphData\")\n",
    "\n",
    "# Clear any files within json_folder before proceeding\n",
    "for file in glob.glob(os.path.join(json_folder, \"*\")):\n",
    "    os.remove(file)\n",
    "\n",
    "for stat in stats:\n",
    "    graphData(stat)\n",
    "json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "merged_json = {}\n",
    "for file_path in json_files:\n",
    "    key = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    with open(file_path, \"r\") as f:\n",
    "        merged_json[key] = json.load(f)\n",
    "\n",
    "with open(os.path.join(json_folder, \"all_data.json\"), \"w\") as out_file:\n",
    "    json.dump(merged_json, out_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6d04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
