{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9536ac",
   "metadata": {},
   "source": [
    "xclim\n",
    "https://xclim.readthedocs.io/en/stable/indicators.html\n",
    "\n",
    "CEDA dataset\n",
    "https://catalogue.ceda.ac.uk/uuid/6a44fecc0f3842faaea53ab617dd2047/?q=&sort_by=title_asc&results_per_page=20&objects_related_to_uuid=6a44fecc0f3842faaea53ab617dd2047&&record_type=Observation&jump=related-anchor\n",
    "\n",
    "CEDA Download\n",
    "https://dap.ceda.ac.uk/badc/ukcp18/data/land-cpm/uk/5km/rcp85/01/\n",
    "\n",
    "UKCP18 Doc\n",
    "https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/research/ukcp/ukcp18-guidance-data-availability-access-and-formats.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "738dbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterio import features\n",
    "import rasterio\n",
    "import os\n",
    "from shapely.geometry import shape, box, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "import json\n",
    "import xclim.indicators.atmos as xci\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xclim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8edbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [\"max\", \"min\", \"med\"]\n",
    "variants = [\"tas\", \"tasmas\", \"tasmin\"]\n",
    "rolling_period = 30\n",
    "output_folder = \"./processed/tas\"\n",
    "input_folder = \"./raw/tas\"\n",
    "models = [\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\", \"23\", \"25\", \"27\", \"29\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd2c4b",
   "metadata": {},
   "source": [
    "### Set up metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203c648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_metric(metric, merged_ds):\n",
    "    metric = metric.groupby('time.year').mean(dim='time', keep_attrs=True)\n",
    "    metric = metric.isel(year=slice(1, None))\n",
    "    metric = metric.transpose(*merged_ds['min_temp'].dims)\n",
    "    metric = metric.assign_coords({\n",
    "            \"projection_y_coordinate\": merged_ds.projection_y_coordinate,\n",
    "            \"projection_x_coordinate\": merged_ds.projection_x_coordinate,\n",
    "            \"year\": merged_ds.year\n",
    "        })\n",
    "    metric.attrs.update(merged_ds[\"min_temp\"].attrs)\n",
    "    return metric\n",
    "\n",
    "def combine_and_extract(model_number):\n",
    "    start_time = time.time()\n",
    "    print(\"Started loading files:\", time.time() - start_time, \"seconds\")\n",
    "    print(f\"Processing model {model_number}\")\n",
    "    tas_files = glob.glob(os.path.join(input_folder,model_number, \"tas_*.nc\"))\n",
    "    tasmax_files = glob.glob(os.path.join(input_folder,model_number, \"tasmax_*.nc\"))\n",
    "    tasmin_files = glob.glob(os.path.join(input_folder,model_number, \"tasmin_*.nc\"))\n",
    "    all_tas = tas_files + tasmax_files + tasmin_files\n",
    "    combined_ds = xr.open_mfdataset(all_tas, combine='by_coords')\n",
    "\n",
    "    yearly_groups = []\n",
    "    for i, (year, yearly_ds) in enumerate(combined_ds.groupby('time.year')):\n",
    "        augmented_yearly_data = yearly_ds.mean(dim='time', keep_attrs=True)        \n",
    "        augmented_yearly_data[\"min_temp\"] = yearly_ds['tasmin'].min(dim='time', keep_attrs=True)\n",
    "        augmented_yearly_data[\"max_temp\"] = yearly_ds['tasmax'].max(dim='time', keep_attrs=True)\n",
    "        yearly_groups.append(augmented_yearly_data)\n",
    "    \n",
    "    print(\"Processed yearly groups:\", time.time() - start_time, \"seconds\")\n",
    "    yearly_groups = yearly_groups[1:]\n",
    "    merged_ds = xr.concat(yearly_groups, dim='year')\n",
    "    combined_ds.close()\n",
    "\n",
    "    merged_ds['rolling_min_temp'] = merged_ds['min_temp'].rolling(year=30, center=True, min_periods=1).mean()\n",
    "    merged_ds['rolling_max_temp'] = merged_ds['max_temp'].rolling(year=30, center=True, min_periods=1).mean()\n",
    "\n",
    "    cdda=xci.cooling_degree_days_approximation(\n",
    "        tas=combined_ds['tas'], \n",
    "        tasmax=combined_ds['tasmax'],\n",
    "        tasmin=combined_ds['tasmin'],\n",
    "        freq='YS',\n",
    "        thresh=\"20.0 degC\"\n",
    "    )\n",
    "    merged_ds['cooling_degree_days'] = fix_metric(cdda, merged_ds)\n",
    "    merged_ds['rolling_cdd'] = merged_ds['cooling_degree_days'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    # print(\"Processed cooling degree days:\", time.time() - start_time, \"seconds\")\n",
    "    \n",
    "    hdda=xci.heating_degree_days_approximation(\n",
    "        tas=combined_ds['tas'], \n",
    "        tasmax=combined_ds['tasmax'],\n",
    "        tasmin=combined_ds['tasmin'],\n",
    "        freq='YS',\n",
    "        thresh=\"15.5 degC\"\n",
    "    )\n",
    "    merged_ds['heating_degree_days'] = fix_metric(hdda, merged_ds)\n",
    "    merged_ds['rolling_hdd'] = merged_ds['heating_degree_days'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    # print(\"Processed heating degree days:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "    hwf = xci.heat_wave_frequency(\n",
    "        tasmax=combined_ds['tasmax'],\n",
    "        tasmin=combined_ds['tasmin'],\n",
    "        freq='YS',\n",
    "        thresh_tasmax=\"26.5 degC\",\n",
    "        thresh_tasmin='10.0 degC',\n",
    "        window=3\n",
    "    )\n",
    "    merged_ds['heatwave_freq'] = fix_metric(hwf, merged_ds)\n",
    "    merged_ds['rolling_hw_freq'] = merged_ds['heatwave_freq'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    # print(\"Processed heatwave frequency:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "    hwl = xci.heat_wave_max_length(\n",
    "        tasmax=combined_ds['tasmax'],\n",
    "        tasmin=combined_ds['tasmin'],\n",
    "        freq='YS',\n",
    "        thresh_tasmax=\"26.5 degC\",\n",
    "        thresh_tasmin='10.0 degC',\n",
    "    )\n",
    "    merged_ds['heatwave_length'] = fix_metric(hwl, merged_ds)\n",
    "    merged_ds['rolling_hw_length'] = merged_ds['heatwave_length'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    # print(\"Processed heatwave length:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "    cfd = xci.consecutive_frost_days(\n",
    "        tasmin=combined_ds['tasmin'],\n",
    "    )\n",
    "\n",
    "    merged_ds['conseq_frost_days'] = fix_metric(cfd, merged_ds)\n",
    "    merged_ds['rolling_cfd'] = merged_ds['conseq_frost_days'].rolling(year=rolling_period, center=True, min_periods=1).mean()\n",
    "    print(\"Completed metrics:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "    merged_ds = merged_ds.drop_vars(['tas', 'tasmax', 'tasmin', 'conseq_frost_days', 'heatwave_length', 'heatwave_freq','heating_degree_days', 'cooling_degree_days','min_temp','max_temp'])\n",
    "    out_rolling_path = os.path.join(output_folder, '01_rolling', f\"rolling_average_{model_number}.nc\")\n",
    "    merged_ds.to_netcdf(out_rolling_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a006c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started loading files: 0.0 seconds\n",
      "Processing model 27\n",
      "Processed yearly groups: 7.474636554718018 seconds\n",
      "Completed metrics: 26.47073006629944 seconds\n",
      "Started loading files: 0.0 seconds\n",
      "Processing model 29\n",
      "Processed yearly groups: 6.881790399551392 seconds\n",
      "Completed metrics: 28.939653873443604 seconds\n"
     ]
    }
   ],
   "source": [
    "select_models = models[14:16]\n",
    "\n",
    "for model in select_models:\n",
    "    combine_and_extract(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c392c",
   "metadata": {},
   "source": [
    "### Characterise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1dcdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterise_rolling_averages():\n",
    "    # Step 1: Load all NetCDF files\n",
    "    file_paths = glob.glob(\"./processed/tas/01_rolling/*.nc\")  # Replace with your actual directory\n",
    "\n",
    "    # Step 2: Open them as a multi-file dataset (combine along 'ensemble_member')\n",
    "    datasets = [xr.open_dataset(fp, decode_times=False) for fp in file_paths]\n",
    "    combined = xr.concat(datasets, dim='ensemble_member')\n",
    "\n",
    "    # Step 3: Compute statistics across the ensemble dimension\n",
    "    median_ds = combined.median(dim='ensemble_member', keep_attrs=True)\n",
    "    min_ds = combined.min(dim='ensemble_member', keep_attrs=True)\n",
    "    max_ds = combined.max(dim='ensemble_member', keep_attrs=True)\n",
    "\n",
    "    # Step 4: Save each result to new .nc files\n",
    "    median_ds.to_netcdf(\"./processed/tas/01-1_stats/temp_median.nc\")\n",
    "    min_ds.to_netcdf(\"./processed/tas/01-1_stats/temp_min.nc\")\n",
    "    max_ds.to_netcdf(\"./processed/tas/01-1_stats/temp_max.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27c651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "characterise_rolling_averages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc99c98",
   "metadata": {},
   "source": [
    "### Manual processing\n",
    "\n",
    "Before running the next steps, load the created .nc file into QGIS, use the batch function to clip the layers, without manually reprojecting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69be9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_cells_to_features(image, transform):\n",
    "    features = []\n",
    "    rows, cols = image.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            value = image[row, col]\n",
    "            if np.isnan(value):\n",
    "                continue\n",
    "            # Get the bounds of the cell\n",
    "            x0, y0 = rasterio.transform.xy(transform, row, col, offset='ul')\n",
    "            x1, y1 = rasterio.transform.xy(transform, row, col, offset='lr')\n",
    "            geom = box(x0, y1, x1, y0)\n",
    "            features.append({\n",
    "                'geometry': mapping(geom),\n",
    "                'properties': {'value': float(value)}\n",
    "            })\n",
    "    return features\n",
    "\n",
    "def tif_to_geojson(tif_folder, geojson_folder):\n",
    "    absYears = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    os.makedirs(geojson_folder, exist_ok=True)\n",
    "    tif_files = glob.glob(os.path.join(tif_folder, \"*.tif\"))\n",
    "    \n",
    "    for tif_file in tif_files:\n",
    "        years = absYears\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            print(f\"\\nProcessing {tif_file}\")\n",
    "\n",
    "            if src.count > 1:\n",
    "                timebands = [src.descriptions[i] if src.descriptions[i] else f\"{i+1}\" for i in range(src.count)]\n",
    "            else:\n",
    "                timeband = src.tags().get('timeband', None)\n",
    "                timebands = [timeband if timeband else \"unknown\"]\n",
    "\n",
    "            selected_indices = [i for i, tb in enumerate(timebands) if any(str(y) in str(tb) for y in years)]\n",
    "\n",
    "            for band_idx in selected_indices:\n",
    "                image = src.read(band_idx + 1)\n",
    "\n",
    "                if src.nodata is not None:\n",
    "                    mask = ~np.isnan(image) & (image != src.nodata)\n",
    "                else:\n",
    "                    mask = ~np.isnan(image)\n",
    "                geoms = raster_cells_to_features(image, src.transform)\n",
    "                gdf = gpd.GeoDataFrame.from_features(geoms, crs=src.crs)\n",
    "                gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "                geojson_path = os.path.join(\n",
    "                    geojson_folder,\n",
    "                    f\"{os.path.splitext(os.path.basename(tif_file))[0]}_{timebands[band_idx]}.geojson\"\n",
    "                )\n",
    "                gdf.to_file(geojson_path, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d5e0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_cdd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_cfd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_hdd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_hwfreq.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_hwlen.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_maxT.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_max_minT.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_cdd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_cfd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_hdd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_hwfreq.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_hwlen.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_maxT.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_med_minT.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_cdd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_cfd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_hdd.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_hwfreq.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_hwlen.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_maxT.tif\n",
      "\n",
      "Processing ./processed/tas\\02_clipped\\TAS_min_minT.tif\n"
     ]
    }
   ],
   "source": [
    "# Converts the reprojected and clipped tif files into geojson files at the different time steps\n",
    "\n",
    "tif_to_geojson(os.path.join(output_folder, \"02_clipped\"), os.path.join(output_folder, \"03_geojson\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e224e",
   "metadata": {},
   "source": [
    "### Inverts JSONS\n",
    "Inverts the storage of data so that it uses lat/lon as keys to reference all the values temporaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "093a140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseGeojson(filename, stat):\n",
    "    file_paths = [\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_10.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_20.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_30.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_40.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_50.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_60.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_70.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_80.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_90.geojson\"),\n",
    "    os.path.join(output_folder, \"03_geojson\", f\"{filename}_100.geojson\"),\n",
    "]\n",
    "\n",
    "    file_labels = [\n",
    "        \"1990\",\n",
    "        \"2000\",\n",
    "        \"2010\",\n",
    "        \"2020\",\n",
    "        \"2030\",\n",
    "        \"2040\",\n",
    "        \"2050\",\n",
    "        \"2060\",\n",
    "        \"2070\",\n",
    "        \"2080\"\n",
    "    ]\n",
    "\n",
    "    # Dictionary to store: { coordinate: [value1, value2, ...] }\n",
    "    coordinate_data = defaultdict(lambda: [None] * len(file_paths))\n",
    "\n",
    "    # Process each file\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        with open(file_path, 'r') as f:\n",
    "            geojson = json.load(f)\n",
    "            for feature in geojson.get(\"features\", []):\n",
    "                geometry = feature.get(\"geometry\", {})\n",
    "                properties = feature.get(\"properties\", {})\n",
    "                value = properties.get(\"value\")  # change if your key is different\n",
    "\n",
    "                if geometry.get(\"type\") == \"Polygon\":\n",
    "                    coords = geometry.get(\"coordinates\", [])\n",
    "                    if coords and coords[0]:  # outer ring exists\n",
    "                        raw_coord = coords[0][0]  # first coordinate of outer ring\n",
    "                        # Format coordinate to 10 decimal places, pad with zeros if necessary\n",
    "                        coord_key = tuple([f\"{c:.10f}\" for c in raw_coord])\n",
    "                        coordinate_data[coord_key][idx] = value\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\"Coordinate\": coord, **{file_labels[i]: values[i] for i in range(len(file_labels))}}\n",
    "        for coord, values in coordinate_data.items()\n",
    "    ])\n",
    "\n",
    "    # Optional: remove rows where all values are None\n",
    "    # df = df.dropna(subset=file_labels, how='all')\n",
    "\n",
    "    # Show result\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.join(output_folder, f\"04_inverse/{stat}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df.to_json(os.path.join(output_dir, f\"{filename}_inverse.json\"), orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e1ba424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stat in stats:\n",
    "    inverseGeojson(f\"TAS_{stat}_cdd\", stat)\n",
    "    inverseGeojson(f\"TAS_{stat}_cfd\", stat)\n",
    "    inverseGeojson(f\"TAS_{stat}_hdd\", stat)\n",
    "    inverseGeojson(f\"TAS_{stat}_hwfreq\", stat)\n",
    "    inverseGeojson(f\"TAS_{stat}_hwlen\", stat)\n",
    "    inverseGeojson(f\"TAS_{stat}_maxT\", stat)\n",
    "    inverseGeojson(f\"TAS_{stat}_minT\", stat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bc949",
   "metadata": {},
   "source": [
    "### Merging JSONS for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "967ac67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphData(stat):\n",
    "    file_paths = {\n",
    "        \"TAS_cdd\": os.path.join(output_folder, \"04_inverse\", stat,f\"TAS_{stat}_cdd_inverse.json\"),\n",
    "        \"TAS_cfd\":os.path.join(output_folder, \"04_inverse\", stat, f\"TAS_{stat}_cfd_inverse.json\"),\n",
    "        \"TAS_hdd\":os.path.join(output_folder, \"04_inverse\", stat, f\"TAS_{stat}_hdd_inverse.json\"),\n",
    "        \"TAS_hwfreq\":os.path.join(output_folder, \"04_inverse\", stat, f\"TAS_{stat}_hwfreq_inverse.json\"),\n",
    "        \"TAS_hwlen\":os.path.join(output_folder, \"04_inverse\", stat, f\"TAS_{stat}_hwlen_inverse.json\"),\n",
    "        \"TAS_maxT\":os.path.join(output_folder, \"04_inverse\", stat, f\"TAS_{stat}_maxT_inverse.json\"),\n",
    "        \"TAS_minT\":os.path.join(output_folder, \"04_inverse\", stat, f\"TAS_{stat}_minT_inverse.json\"),\n",
    "    }\n",
    "    \n",
    "    data_by_metric = {}\n",
    "\n",
    "    # Load each file and organize by coordinate\n",
    "    for metric, path in file_paths.items():\n",
    "        with open(path, 'r') as f:\n",
    "            records = json.load(f)\n",
    "            for entry in records:\n",
    "                # Convert coordinate list to tuple for use as a dictionary key\n",
    "                coord_key = tuple(entry[\"Coordinate\"])\n",
    "                if coord_key not in data_by_metric:\n",
    "                    data_by_metric[coord_key] = {}\n",
    "                # Extract values for 1990 to 2080 in 10-year steps\n",
    "                values = [entry.get(str(year)) for year in range(1990, 2090, 10)]\n",
    "                data_by_metric[coord_key][metric] = values\n",
    "\n",
    "    # Convert coordinate tuples to strings for JSON serialization\n",
    "    final_output = {str(k): v for k, v in data_by_metric.items()}\n",
    "\n",
    "    # Save the final merged dictionary as a new JSON file\n",
    "    output_file = os.path.join(output_folder, f\"05_graphData/{stat}.json\")\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        json.dump(final_output, out_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45ddd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = os.path.join(output_folder, \"05_graphData\")\n",
    "\n",
    "# Clear any files within json_folder before proceeding\n",
    "for file in glob.glob(os.path.join(json_folder, \"*\")):\n",
    "    os.remove(file)\n",
    "\n",
    "for stat in stats:\n",
    "    graphData(stat)\n",
    "json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "merged_json = {}\n",
    "for file_path in json_files:\n",
    "    key = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    with open(file_path, \"r\") as f:\n",
    "        merged_json[key] = json.load(f)\n",
    "\n",
    "with open(os.path.join(json_folder, \"all_data.json\"), \"w\") as out_file:\n",
    "    json.dump(merged_json, out_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16950130",
   "metadata": {},
   "source": [
    "### Merge the two all_data.json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b5712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_path = './processed/pr/05_graphData/all_data.json'\n",
    "tas_data_path = './processed/tas/05_graphData/all_data.json'\n",
    "output_path = './processed/all_data.json'\n",
    "\n",
    "# Load both JSON files\n",
    "with open(pr_data_path, 'r') as f:\n",
    "    pr_json = json.load(f)\n",
    "with open(tas_data_path, 'r') as f:\n",
    "    tas_json = json.load(f)\n",
    "\n",
    "# Merge the two dictionaries\n",
    "merged_json = {}\n",
    "\n",
    "# Add all keys from tas_json\n",
    "for stat_key, stat_data in tas_json.items():\n",
    "    merged_json[stat_key] = stat_data\n",
    "\n",
    "# Add all keys from pr_json (if key exists, merge metrics; else, add new)\n",
    "for stat_key, stat_data in pr_json.items():\n",
    "    if stat_key in merged_json:\n",
    "        # Merge metrics for each coordinate\n",
    "        for coord_key, metrics in stat_data.items():\n",
    "            if coord_key in merged_json[stat_key]:\n",
    "                merged_json[stat_key][coord_key].update(metrics)\n",
    "            else:\n",
    "                merged_json[stat_key][coord_key] = metrics\n",
    "    else:\n",
    "        merged_json[stat_key] = stat_data\n",
    "\n",
    "# Save merged result\n",
    "with open(output_path, 'w') as out_file:\n",
    "    json.dump(merged_json, out_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
